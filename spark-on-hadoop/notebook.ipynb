{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843323dc",
   "metadata": {},
   "source": [
    "### Lesson #1: There is no inherent Spark version in Hadoop YARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyspark==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('yarn')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43896b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ae4b4",
   "metadata": {},
   "source": [
    "### Lesson #2: Python UDFs require Python interpreters, duh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a16549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_python_version():\n",
    "    import sys\n",
    "    return f'{sys.executable} v{sys.version}'\n",
    "py_udf = udf(get_python_version)\n",
    "spark.range(3).withColumn(\"newCol\", py_udf()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542abcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('yarn')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28dec4",
   "metadata": {},
   "source": [
    "### Lesson #3: It’s Bring Your Own Python to Cluster Day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c4984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/opt/conda/bin/conda install -y conda-pack\n",
    "!/opt/conda/bin/conda create -y python=3.8.13 --name=conda-env\n",
    "!/opt/conda/bin/conda pack --prefix /opt/conda/envs/conda-env -f -o ./conda-env.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0be04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = './conda-env/bin/python3.8'\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('yarn')\n",
    "    .config('spark.yarn.dist.archives','./conda-env.tgz#conda-env')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_python_version():\n",
    "    import sys\n",
    "    return f'{sys.executable} v{sys.version}'\n",
    "py_udf = udf(get_python_version)\n",
    "spark.range(3).withColumn(\"newCol\", py_udf()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a25da",
   "metadata": {},
   "source": [
    "### Lesson #4: PyPI packages hitch a ride when distributing conda environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9224d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/conda/envs/conda-env/bin/pip install pandas==1.4.2\n",
    "!/opt/conda/bin/conda pack --prefix /opt/conda/envs/conda-env -f -o ./conda-env.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f396e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = './conda-env/bin/python3.8'\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('yarn')\n",
    "    .config('spark.yarn.dist.archives','./conda-env.tgz#conda-env')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_pandas_version():\n",
    "    import pandas\n",
    "    return f'pandas v{pandas.__version__}'\n",
    "py_udf = udf(get_pandas_version)\n",
    "spark.range(3).withColumn(\"newCol\", py_udf()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4736cd2",
   "metadata": {},
   "source": [
    "### Lesson #5: New data source? There’s a Jar for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.2.jar -o bq.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf301bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('yarn')\n",
    "    .config('spark.yarn.dist.jars','./bq.jar')\n",
    "    .config('spark.driver.extraClassPath','./bq.jar')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e39f3",
   "metadata": {},
   "source": [
    "`gcloud auth application-default print-access-token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .option(\"gcpAccessToken\", '<your token here>')\n",
    "      .option(\"parentProject\", \"<your GCP project here>\")\n",
    "      .format(\"bigquery\")\n",
    "      .load(\"bigquery-public-data.samples.shakespeare\")\n",
    "     )\n",
    "df.groupBy('corpus') \\\n",
    "  .sum('word_count') \\\n",
    "  .orderBy('sum(word_count)', ascending=False) \\\n",
    "  .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93376b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('yarn')\n",
    "    .config('spark.jars.packages','com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.24.2')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e95c1d",
   "metadata": {},
   "source": [
    "### Lesson #6: It’s dangerous to go alone! Take your helper.py along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./helper.py', 'w') as f:\n",
    "    f.write(\n",
    "'''def get_module():\n",
    "    return __name__\n",
    "''')\n",
    "with open('./utils.py', 'w') as f:\n",
    "    f.write(\n",
    "'''def get_module():\n",
    "    return __name__\n",
    "''')\n",
    "import helper\n",
    "import utils\n",
    "f'{helper.get_module()}, {utils.get_module()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ce1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('yarn')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fabadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip friends.zip utils.py helper.py\n",
    "spark.sparkContext.addPyFile('./friends.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4159f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_module_names():\n",
    "    import utils\n",
    "    import helper\n",
    "    return f'{helper.get_module()}, {utils.get_module()}'\n",
    "py_udf = udf(get_module_names)\n",
    "spark.range(3).withColumn(\"newCol\", py_udf()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532929e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
